---
title: "Polygenic Multiplicative Fitness & MCMC"
# subtitle: "EQUATIONS"
author: "Moi Exposito-Alonso"
date: '`r Sys.Date()`'
output:
  pdf_document:
  header-inclusdes:
   - \usepackage{amsmath}
   - \usepackage{caption}
   - \usepackage{subcaption}
   - \usepackage{graphicx}
  #html_document: default
  #header-includes:
  #- \setlength{\parindent}{4em}
  #- \setlength{\parskip}{0em}
---

<!-- https://en.wikipedia.org/wiki/Normal_distribution -->

<!-- https://en.wikipedia.org/wiki/Gamma_distribution -->

<!-- https://onlinecourses.science.psu.edu/stat414/node/191 -->

<!-- https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood -->

<!-- 
COMMENTS

Use BFGS 
Maybe visit GUSFIELD in Davis

A poisson or overdispersion likelihood
-->

<!-- *********************************************************************** -->
### Genotypes and selection model 

Given two loci, A and B, with alleles A and a, and B and b, there must be nine possible gammetes. If the A allele and B allele are selected with coefficients $s_{A}$ and $s_{B}$, and we assume no dominance and no epistasis, we can write the average fitness per genotype. For simplicity we use an haploid model, which would be similar to the diploid homozygotes case (also the case of highly self-fertilizing species):

\
\begin{center}
\begin{tabular}{ c | c  c}
     &       B            &     b       \\
  \hline
  A & $(1+s_A)(1+s_B) $&$ (1+s_{A}) (1-s_{B})  $\\
  a & $(1- s_{A})(1+ s_{B})$&$ (1-s_A)(1-s_B)          $
\end{tabular}
\end{center}
\

<!-- \ -->
<!-- \begin{center} -->
<!-- \begin{tabular}{ c | c  c} -->
<!--      &       B            &     b       \\  -->
<!--   \hline -->
<!--   A & $(1+s_A)(1+s_B) \times \epsilon $&$ (1+s_{A}) (1-s_{B})  $\\   -->
<!--   a & $(1- s_{A})(1+ s_{B})$&$ (1-s_A)(1-s_B) \times \epsilon          $     -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- \ -->

<!-- \ -->
<!-- \begin{center} -->
<!-- \begin{tabular}{ c | c  c} -->
<!--      &       B            &     b       \\  -->
<!--   \hline -->
<!--   A & $[(1+s_A)(1+s_B)]^{\epsilon} $&$ [(1+s_{A}) (1-s_{B})]^e  $\\   -->
<!--   a & $[(1- s_{A})(1+ s_{B})]^e$&$ [(1-s_A)(1-s_B)]^{\epsilon}          $     -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- \ -->


We could then expect that the observed fitness $y$ of one of $N$ number of haplotype of $p$ loci is expressed as:

<!-- \odot -->

$$ y_h = w(s,X) = \prod_{i=1}^{p} (1+s_i X_{i,h})  $$

<!-- *********************************************************************** -->
### The probabilistic model 

The observed fitness of a haplotype will have a sampling variance that follows a Gamma distribution (typically with $\alpha$ and $beta$ parameters or $\alpha = k$ and $\theta = 1/\beta$):

$$ Y  \sim \Gamma ( \alpha , \beta )  $$


The first moment is:

$$ E[Y] = \alpha / \beta $$

The second moment is then:

$$ Var(y) = E[Y^2]-E[Y] =  \alpha / \beta^2 $$
We can take the second moment as a constant that we can calculate from the observed variane across replicates, $v$. The first moment, $\mu$ will be the function $w(s,X)$ dependent on the genotypes and selection coefficients. Then we can rewrite the sape and rate parameters in terms of the moments $\alpha = \mu^2 /v$ and $\beta = \mu/v$.

\

The probability density function of Gamma in terms of $k=alpha$ and $\theta=1/ \beta$ is:

$$f(y) = \frac{1}{\Gamma(k) \theta^{k}} y^{k-1} e^{-\frac{y}{\theta}}$$
In log form this can be written as:


$$\ln f(y) = (k-1) \ln(y) {-\frac{y}{\theta}} - \ln \Gamma(k) - k \ln \theta  $$
And in terms of moments:

$$ \ln f(y) = (\frac{\mu^2}{v}-1) \ln(y) {- y \frac{\mu}{v}} - \ln \Gamma(\frac{\mu^2}{v}) - \frac{\mu^2}{v} \ln \frac{v}{\mu}  $$

And in terms of selection coefficients and genotypes

\begin{align}
\ln f(y) &= \nonumber \\
  & (\frac{\prod_{i=1}^{p} (1+s_i  X_{i,h})^2}{v}-1) \ln(y)  \nonumber \\
  & - y \frac{\prod_{i=1}^{p} (1+s_i  X_{i,h})}{v} \nonumber \\
  & - \ln \Gamma(\frac{\prod_{i=1}^{p} (1+s_i  X_{i,h})^2}{v}) \nonumber \\
  & - \frac{\prod_{i=1}^{p} (1+s_i  X_{i,h})^2}{v} \ln \frac{v}{\prod_{i=1}^{p} (1+s_i  X_{i,h})}  
\end{align}



For the full log likelihood we sum over all haplotypes:

$$ \ln \mathcal {L}(\vec{s})  = \sum_{h \in haps} \ln f(y \ | X_h, \vec{s},v_h)$$

### Prior

We assume that the array of selection coefficients follow a multivariate Normal:

<!-- $$\vec{s}  \sim \mathcal{uniform(-5,5)} $$ -->
$$\vec{s} =(s_1, ..., s_n) \sim \mathcal{N}(u,v)$$
So the prior probability is:
$$f(s) = \frac {1}{\sqrt {2\pi v^{2}}} e^{- \frac{(s-u )^2}{2v^{2}}}$$
And the log probability for all selection coefficients:

$$ \ln f(\vec{s}) = -{\frac {p}{2}}\ln(2\pi )-{\frac {p}{2}}\ln v^{2}-{\frac {1}{2v^{2}}}\sum _{i=1}^{p}(s_{i}-u )^{2} $$

$$\ln \mathcal{L}(\vec{s}) = \sum_{i=1}^{p} \ln f(s_{i};\,u ,v) $$

### The conditional probability


$$\mathcal{P}(\vec{s},u,v | y) = \mathcal{L}(y |\vec{s}) + \ln \mathcal{L}(\vec{s}|u,v) $$

### MCMC

The updating/acceptance scheme is:

$$min \{ \ 1, \frac{  \mathcal{P}(y, \vec{s}^*, \alpha^*,\beta^*)q(\vec{s}^*, \vec{s}) } {       \mathcal{P}(y, \vec{s}, \alpha,\beta)q(\vec{s}^,\vec{s}^*)} \ \   \ \} $$

By having a symetric transition probability matrix (the Metropolis method), i.e. $q(\vec{s}^*, s) = q(\vec{s}^*, s)$, we can express the acceptance just as $$\frac{  \mathcal{P}(y, \vec{s}^*, \alpha^*,\beta^*)} {  \mathcal{P}(y, \vec{s}, \alpha,\beta)} $$


After running a number of steps, an stationary distribution is obtained.



<!-- //////////////////////////////////////////////////////////////////////// -->
<!-- //////////////////////////////////////////////////////////////////////// -->

<!-- $$ - \ln \ell(\vec{s}, e) = - \ln f(y_h \ ; \sigma )$$ -->

<!-- If we wanted to apply this to dense data, we can use some regulaization. $L_1= \lambda ||w||_1$, $L_2=\lambda ||w||_2^2$ norms, or a combination, $L_{12} = \lambda( m ||w||_1 + (1-m) ||w||_{2}^{2}), m \in [0,1]$ typically used from regression approaches could be used. A plane $L_0$ norm equal to the total number of non-zero coefficients could be adapted to the percentage of non-zero values thus $L_0 \in [0,1]$, which in log would be of the same scale as the likelihood function.  -->

<!-- $$ - \ln \ell(\vec{s}, e)  \ \ +  \ln (L_{0})$$ -->


<!-- And the negative log likelihood: -->

<!-- $$ \ell (k,\theta )=(k-1)\sum _{i=1}^{N}\ln {(x_{i})}-\sum _{i=1}^{N}{\frac {x_{i}}{\theta }}-Nk\ln(\theta )-N\ln(\Gamma (k)) $$ -->
<!-- Which in terms of $\sigma$ can be writen as: -->

<!-- $$ \ell (\sigma)=(\sigma-1)\sum _{i=1}^{N}\ln {(x_{i})}-\sum _{i=1}^{N}{x_{i}\sigma}-N \sigma \ln(\sigma^{-1} )-N\ln(\Gamma (\sigma)) $$ -->

<!-- And because $\sigma$ is a function of the genotypes, selection coefficients, and epistatic term, we can substite it and generate the full likelihood: -->


<!-- \begin{align} -->
<!--    \ell (s,e) &=\prod_{h \in haps.}^{N} \Bigg[ -->
<!--               & (\frac{1}{ \prod_{i=1}^{p} (1+s_i X_{h,i}) ^{2e}- 1}-1) \sum_{i=1}^{N}\ln {(y_{i})}- \\ -->
<!--               & \sum_{i=1}^{N}{\frac{y_{i}}{ \prod_{i=1}^{p} (1+s_i X_{h,i}) ^{2e}- 1}} - \\ -->
<!--               & \frac{N}{ \prod_{i=1}^{p} (1+s_i X_{h,i}) ^{2e}- 1} \ln(\prod_{i=1}^{p} (1+s_i X_{h,i}) ^{2e}- 1)- \\ -->
<!--               & N\ln(\Gamma (\frac{1}{ \prod_{i=1}^{p} (1+s_i X_{h,i}) ^{2e}- 1}))  \Bigg] -->
<!-- \end{align} -->


<!-- ### Extension where $e$ is a random variable (needs to be worked out) -->

<!-- An important extension of this method would be to instead of estimating a common epistatic term for the whole genome, we infer its ditribution. -->

<!-- To do that we integrate the likelihood over the distribution of e given some parameters $\theta_e$ of such distribution with known probability denstiy function. -->


<!-- $$ \ln \ell(\vec{s},\theta_e ) = \int_{-\inf}^{+\inf}  \ln \ell(\vec{s}, e) f(e; \theta_e) de $$ -->

<!-- Or for feasibility, the $e$ distribution can be bined in equal sizes and approximate: -->

<!-- $$ \ln \ell(\vec{s},\theta_e ) \approx \sum_{m \in e \ bins}  \ln \ell(\vec{s}, e) P(e; \theta_e)  $$ -->
<!-- The distribution of $e$ should be optimally a [conjugate](https://en.wikipedia.org/wiki/Conjugate_prior) of the likelihood. -->



<!--
################################################################################
## Showing how well it fits with real data
################################################################################
-->

<!-- In order to show that this distribution is appropriate, here are two relative fitnesses, in Madrid under drought and high water conditions. Only by moving $\sigma$ we obtain almot perfect fit. -->

<!-- ```{r,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results='hide',fig.width=4,fig.align='center'} -->
<!-- require(ggplot2) -->
<!-- require(cowplot) -->
<!-- require(latex2exp) -->
<!-- require(dplyr) -->
<!-- require(gws) -->

<!-- # for(sig in c(0.001,seq(0.01,5,length.out = 10) )){ -->
<!-- #   if(sig==0.01){ -->
<!-- #     myplot<-qplot(rgamma(100000,shape=sig,rate = sig),geom='density') -->
<!-- #   }else{ -->
<!-- #     myplot<-myplot+geom_density(data=data.frame(x=rgamma(100000,shape=sig,rate = sig)), aes(x=x)) -->
<!-- #   } -->
<!-- # } -->

<!-- Yi=dry$Fitness_mli %>% relative -->
<!-- Yp=dry$Fitness_mlp %>% relative -->
<!-- Yi=dry$Fitness_mhp %>% relative -->

<!-- set.seed(1) -->
<!-- ggplot(data=data.frame(Yi,Yp)) + -->
<!--   geom_density(data=data.frame(x=rgamma(515,shape=7,rate = 7)), aes(x))+ -->
<!--   geom_density(data=data.frame(x=rgamma(515,shape=0.3,rate = 0.3)), aes(x))+ -->
<!--   geom_density(aes(x=Yp),fill=transparent('firebrick3'),color=transparent('black',0))+ -->
<!--   geom_density(aes(x=Yi),fill=transparent('navy'),color=transparent('black',0))+ -->
<!--   xlab('relative fitness')+ -->
<!--   ggtitle(TeX( 'with $\\sigma = 0.3$ and 6 for $\\Gamma$ distribution')) -->
<!--   # geom_density(data=data.frame(x=rgamma(515,shape=0.5,rate = 0.5)), aes(x))+ -->


<!-- ``` -->
